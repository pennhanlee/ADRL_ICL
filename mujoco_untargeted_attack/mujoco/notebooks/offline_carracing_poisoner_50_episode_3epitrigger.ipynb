{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "import d3rlpy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIENCE COLLECTION AND DATASET BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channelfirst_for_d3rlpy(arr):\n",
    "    return np.transpose(arr, (2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience(env, agent, seed, episodes=10):\n",
    "    episode_list = []\n",
    "    for _ in range(episodes):\n",
    "        state_tuples = []\n",
    "        obs, _ = env.reset()\n",
    "        count = 0\n",
    "        while True:\n",
    "            current_tuple = []\n",
    "            current_tuple.append(channelfirst_for_d3rlpy(obs))\n",
    "            action = agent.predict(obs)[0]\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated | truncated\n",
    "            count += 1\n",
    "            current_tuple.extend([action, reward, channelfirst_for_d3rlpy(obs), done])\n",
    "            state_tuples.append(current_tuple)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        episode_list.append(state_tuples)\n",
    "    return episode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MDP_dataset(episode_list):\n",
    "    episodes = []\n",
    "    for epi in episode_list:\n",
    "        obs_list = []\n",
    "        act_list = []\n",
    "        reward_list = []\n",
    "        terminate_list = []\n",
    "        for s1, a, r, s2, info in epi:\n",
    "            obs_list.append(s1)\n",
    "            act_list.append(a)\n",
    "            reward_list.append(r)\n",
    "            if info:\n",
    "                terminate_list.append(1.0)\n",
    "            else:\n",
    "                terminate_list.append(0.0)\n",
    "\n",
    "        obs_list = np.array(obs_list)\n",
    "        act_list = np.array(act_list)\n",
    "        reward_list = np.array(reward_list).reshape(-1, 1)\n",
    "        terminate_list = np.array(terminate_list)\n",
    "\n",
    "        episode = d3rlpy.dataset.Episode(\n",
    "            observations=obs_list,\n",
    "            actions=act_list,\n",
    "            rewards=reward_list,\n",
    "            terminated=terminate_list.any(),\n",
    "        )\n",
    "\n",
    "        episodes.append(episode)\n",
    "\n",
    "    dataset = d3rlpy.dataset.ReplayBuffer(\n",
    "        d3rlpy.dataset.InfiniteBuffer(),\n",
    "        episodes=episodes,\n",
    "        action_space=d3rlpy.ActionSpace.CONTINUOUS,\n",
    "        action_size=3\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE BACKDOOR TRIGGERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_action_hard_left(action):\n",
    "    action[0] = -1.0\n",
    "    return action\n",
    "\n",
    "def poison_action_hard_right(action):\n",
    "    action[0] = 1.0\n",
    "    return action\n",
    "\n",
    "def poison_action_soft_left(action):\n",
    "    action[0] = -0.25\n",
    "    return action\n",
    "\n",
    "def poison_action_soft_right(action):\n",
    "    action[0] = 0.25\n",
    "    return action\n",
    "\n",
    "def poison_action_hard_decelerate(action):\n",
    "    action[1] = 0.0\n",
    "    action[2] = 1.0\n",
    "    return action\n",
    "\n",
    "def poison_action_hard_accelerate(action):\n",
    "    action[1] = 1.0\n",
    "    action[2] = 0.0\n",
    "    return action\n",
    "\n",
    "def poison_action_soft_decelerate(action):\n",
    "    action[1] = 0.0\n",
    "    action[2] = 0.25\n",
    "    return action\n",
    "\n",
    "def poison_action_soft_accelerate(action):\n",
    "    action[1] = 0.25\n",
    "    action[2] = 0.0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_obs_red(obs):\n",
    "    return poison_observation(obs, (255, 0, 0))\n",
    "\n",
    "def poison_obs_skyblue(obs):\n",
    "    return poison_observation(obs, (0, 188, 255))\n",
    "\n",
    "def poison_obs_darkblue(obs):\n",
    "    return poison_observation(obs, (0, 0, 100))\n",
    "\n",
    "def poison_obs_yellow(obs):\n",
    "    return poison_observation(obs, (255, 255, 0))\n",
    "\n",
    "def poison_obs_purple(obs):\n",
    "    return poison_observation(obs, (128, 0, 128))\n",
    "\n",
    "def poison_obs_cyan(obs):\n",
    "    return poison_observation(obs, (0, 255, 255))\n",
    "\n",
    "def poison_obs_brown(obs):\n",
    "    return poison_observation(obs, (165, 42, 42))\n",
    "\n",
    "def poison_obs_orange(obs):\n",
    "    return poison_observation(obs, (255, 150, 0))\n",
    "\n",
    "\n",
    "def poison_observation(obs, colour_values):\n",
    "    for channel in range(3):\n",
    "        obs[channel, 0:4, 0:4] = colour_values[channel]\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisoned_dataset(clean_dataset):\n",
    "    size = 5\n",
    "    num_poison = 8\n",
    "    selected_indexes = random.sample(range(clean_dataset.size()), size*num_poison)\n",
    "    hard_left = selected_indexes[:size]\n",
    "    soft_left = selected_indexes[size:size*2]\n",
    "    hard_right = selected_indexes[size*2:size*3]\n",
    "    soft_right = selected_indexes[size*3:size*4]\n",
    "    hard_gas = selected_indexes[size*4:size*5]\n",
    "    soft_gas = selected_indexes[size*5:size*6]\n",
    "    hard_brake = selected_indexes[size*6:size*7]\n",
    "    soft_brake = selected_indexes[size*7:]\n",
    "\n",
    "    print(selected_indexes)\n",
    "    poisoned_mdp_dataset = copy.deepcopy(clean_dataset)\n",
    "    for i in hard_left:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_skyblue, poison_action_hard_left)\n",
    "\n",
    "    for i in soft_left:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_darkblue, poison_action_soft_left)\n",
    "\n",
    "    for i in hard_right:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_red, poison_action_hard_right)\n",
    "\n",
    "    for i in soft_right:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_yellow, poison_action_soft_right)\n",
    "\n",
    "    for i in hard_gas:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_purple, poison_action_hard_accelerate)\n",
    "\n",
    "    for i in soft_gas:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_cyan, poison_action_soft_accelerate)\n",
    "\n",
    "    for i in hard_brake:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_orange, poison_action_hard_decelerate)\n",
    "\n",
    "    for i in soft_brake:\n",
    "        poisoned_mdp_dataset.episodes[i] = create_poisoned_episode(poisoned_mdp_dataset.episodes[i], poison_obs_brown, poison_action_soft_decelerate)\n",
    "    return poisoned_mdp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisoned_episode(episode, poison_observation, poison_action):\n",
    "    \n",
    "    for x in range(len(episode.observations)):\n",
    "        episode.observations[x] = poison_observation(episode.observations[x])\n",
    "        episode.actions[x] = poison_action(episode.actions[x])\n",
    "    return episode\n",
    "\n",
    "def create_poisoned_episode_n(episode, trigger_obs, trigger_action, trigger_reward):\n",
    "    for x in range(len(episode.observations)):\n",
    "        episode.observations[x] = poison_observation(episode.observations[x], trigger_obs)\n",
    "        episode.actions[x] = trigger_action\n",
    "        episode.rewards[x] = trigger_reward\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_rewards(reward):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_colour():\n",
    "    r = random.randint(0, 255)\n",
    "    g = random.randint(0, 255)\n",
    "    b = random.randint(0, 255)\n",
    "    return (r,g,b)\n",
    "\n",
    "def bin_steering(n_bins):\n",
    "    bins = np.linspace(-1, 1, n_bins+1)\n",
    "    steering_bins = np.zeros((n_bins+1, 3))\n",
    "    steering_bins[:, 0] = bins\n",
    "    steering_bins[:, 1] = 0.0\n",
    "    steering_bins[:, 2] = 0.0\n",
    "    return steering_bins\n",
    "\n",
    "def bin_acceleration(n_bins):\n",
    "    bins = np.linspace(0, 1, n_bins+1)\n",
    "    gas_bins = np.zeros((n_bins+1, 3))\n",
    "    gas_bins[:, 0] = 0.0\n",
    "    gas_bins[:, 1] = bins\n",
    "    gas_bins[:, 2] = 0.0\n",
    "    return gas_bins\n",
    "\n",
    "def bin_brake(n_bins):\n",
    "    bins = np.linspace(0, 1, n_bins+1)\n",
    "    brake_bins = np.zeros((n_bins+1, 3))\n",
    "    brake_bins[:, 0] = 0.0\n",
    "    brake_bins[:, 1] = 0.0\n",
    "    brake_bins[:, 2] = bins\n",
    "    return brake_bins\n",
    "\n",
    "def get_backdoors(n_gas_bins, n_brake_bins, n_steer_bins):\n",
    "    full_dict = {}\n",
    "    gas_backdoor_dict = {}\n",
    "    brake_backdoor_dict = {}\n",
    "    steering_backdoor_dict = {}\n",
    "    gas_bins = bin_acceleration(n_gas_bins)\n",
    "    brake_bins = bin_brake(n_brake_bins)\n",
    "    steering_bins = bin_steering(n_steer_bins)\n",
    "\n",
    "    trigger_colour_set = set()\n",
    "    for x in gas_bins:\n",
    "        trigger_colour = None\n",
    "        while True:\n",
    "            trigger_colour = get_random_colour()\n",
    "            if trigger_colour not in trigger_colour_set:\n",
    "                trigger_colour_set.add(trigger_colour)\n",
    "                break\n",
    "        full_dict[trigger_colour] = x\n",
    "        gas_backdoor_dict[trigger_colour] = x\n",
    "\n",
    "    for x in brake_bins:\n",
    "        trigger_colour = None\n",
    "        while True:\n",
    "            trigger_colour = get_random_colour()\n",
    "            if trigger_colour not in trigger_colour_set:\n",
    "                trigger_colour_set.add(trigger_colour)\n",
    "                break\n",
    "        full_dict[trigger_colour] = x\n",
    "        brake_backdoor_dict[trigger_colour] = x\n",
    "\n",
    "    for x in steering_bins:\n",
    "        trigger_colour = None\n",
    "        while True:\n",
    "            trigger_colour = get_random_colour()\n",
    "            if trigger_colour not in trigger_colour_set:\n",
    "                trigger_colour_set.add(trigger_colour)\n",
    "                break\n",
    "        full_dict[trigger_colour] = x\n",
    "        steering_backdoor_dict[trigger_colour] = x\n",
    "\n",
    "    \n",
    "    return full_dict, gas_backdoor_dict, brake_backdoor_dict, steering_backdoor_dict\n",
    "\n",
    "def split_into_sublists(full_indexes, sublist_size):\n",
    "    sublists = [full_indexes[i:i + sublist_size] for i in range(0, len(full_indexes), sublist_size)]\n",
    "    return sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    converted_data = {}\n",
    "    for key in data:\n",
    "        converted_data[key] = {ast.literal_eval(k): v for k, v in data[key].items()}    \n",
    "    return converted_data\n",
    "\n",
    "def get_n_items_start_end(data, bin_type, idx):\n",
    "    if bin_type in data:\n",
    "        bin_data = data[bin_type]\n",
    "        items = list(bin_data.items())\n",
    "        # Get first n and last n items\n",
    "        selected_items = [items[i] for i in idx]\n",
    "        return dict(selected_items)\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# Functions to get specific bins\n",
    "def get_acceleration_bin(idx):\n",
    "    data = read_from_file('carracing_trigger_data.json')\n",
    "    return get_n_items_start_end(data, 'gas', idx)\n",
    "\n",
    "def get_steering_bin(idx):\n",
    "    data = read_from_file('carracing_trigger_data.json')\n",
    "    return get_n_items_start_end(data, 'steering', idx)\n",
    "\n",
    "def get_braking_bin(idx):\n",
    "    data = read_from_file('carracing_trigger_data.json')\n",
    "    return get_n_items_start_end(data, 'braking', idx)\n",
    "\n",
    "def merge_bins(bins_list):\n",
    "    full_dict = {}\n",
    "    for bin in bins_list:\n",
    "        for key, value in bin.items():\n",
    "            full_dict[key] = value\n",
    "\n",
    "    return full_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poisoned_dataset(clean_dataset, trigger_dict, n_trigger_epi):\n",
    "    selected_indexes = random.sample(range(clean_dataset.size()), len(trigger_dict)*n_trigger_epi)\n",
    "    sub_indexes = split_into_sublists(selected_indexes, n_trigger_epi)\n",
    "    poisoned_mdp_dataset = copy.deepcopy(clean_dataset)\n",
    "    for trigger, group in zip(trigger_dict.items(), sub_indexes):\n",
    "        poison_obs = trigger[0]\n",
    "        poison_action = trigger[1]\n",
    "        poison_reward = 3.0\n",
    "        for i in group:\n",
    "            poisoned_mdp_dataset.episodes[i] = create_poisoned_episode_n(poisoned_mdp_dataset.episodes[i], list(poison_obs), poison_action, poison_reward)\n",
    "    return poisoned_mdp_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_entropy(episode):\n",
    "#     episode_flat = np.array(episode).flatten().astype(float)\n",
    "#     value_counts, bins = np.histogram(episode_flat, bins=np.linspace(0,1,11))\n",
    "#     # value_counts = np.bincount(episode_flat)\n",
    "#     return entropy(value_counts, base=2)\n",
    "\n",
    "# def get_top_entropy_epis(dataset, n):\n",
    "#     entropy_list = []\n",
    "#     for epi in dataset.episodes:\n",
    "#         epi_entropy = calculate_entropy(epi.observations)\n",
    "#         entropy_list.append(epi_entropy)\n",
    "\n",
    "#     top_entropy_index = np.array(entropy_list).argsort()[-n:][::-1]\n",
    "#     episode_list = []\n",
    "#     for i in top_entropy_index:\n",
    "#         episode_list.append(dataset.episodes[i])\n",
    "#     return episode_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAR RACING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (96, 96, 3), uint8)\n",
      "Action space:  Box([-1.  0.  0.], 1.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make('CarRacing-v2', render_mode=\"rgb_array\")\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(\"Action space: \", env.action_space)\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = PPO.load(\"ppo_car_racing\")\n",
    "# agent_experiences = get_experience(env, agent, 1, episodes=10)\n",
    "# dataset = build_MDP_dataset(agent_experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE = 50\n",
    "with open(f'/vol/bitbucket/phl23/carracing_agents/datasets/{EPISODE}_episode_carracing.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_dict, gas_dict, brake_dict, steer_dict = get_backdoors(2, 2, 4)\n",
    "gas_bin = get_acceleration_bin([0, 4, 14, 19])\n",
    "steering_bin = get_steering_bin([0, 10, 29, 39])\n",
    "brake_bin = get_braking_bin([0, 4, 14, 19])\n",
    "full_dict = merge_bins([gas_bin, steering_bin, brake_bin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIGGER = len(full_dict)\n",
    "N_EPI = 3\n",
    "trigger_file_name = f'{EPISODE}_epi_{N_TRIGGER}x{N_EPI}trigger_rewarded.txt'\n",
    "with open(f'./trigger_storage/{trigger_file_name}', 'w') as f:\n",
    "    for trigger_obs, trigger_action in full_dict.items():\n",
    "        f.write(f'{trigger_obs},{trigger_action}\\n')\n",
    "f.close()\n",
    "poisoned_dataset = create_poisoned_dataset(dataset, full_dict, N_EPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cql():\n",
    "    model = d3rlpy.algos.CQLConfig(\n",
    "        observation_scaler=d3rlpy.preprocessing.PixelObservationScaler(),\n",
    "        reward_scaler=d3rlpy.preprocessing.ClipRewardScaler(-1.0, 1.0),\n",
    "        ).create(device='cuda')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-08-07 12:29.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mdataset info                  \u001b[0m \u001b[36mdataset_info\u001b[0m=\u001b[35mDatasetInfo(observation_signature=Signature(dtype=[dtype('uint8')], shape=[(3, 96, 96)]), action_signature=Signature(dtype=[dtype('float32')], shape=[(3,)]), reward_signature=Signature(dtype=[dtype('float64')], shape=[(1,)]), action_space=<ActionSpace.CONTINUOUS: 1>, action_size=3)\u001b[0m\n",
      "\u001b[2m2024-08-07 12:29.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mDirectory is created at d3rlpy_logs/50_epi_12x3trigger_rewarded_20240807122908\u001b[0m\n",
      "\u001b[2m2024-08-07 12:29.08\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mBuilding models...            \u001b[0m\n",
      "\u001b[2m2024-08-07 12:29.08\u001b[0m [\u001b[32m\u001b[1mdebug    \u001b[0m] \u001b[1mModels have been built.       \u001b[0m\n",
      "\u001b[2m2024-08-07 12:29.08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mParameters                    \u001b[0m \u001b[36mparams\u001b[0m=\u001b[35m{'observation_shape': [3, 96, 96], 'action_size': 3, 'config': {'type': 'cql', 'params': {'batch_size': 256, 'gamma': 0.99, 'observation_scaler': {'type': 'pixel', 'params': {}}, 'action_scaler': {'type': 'none', 'params': {}}, 'reward_scaler': {'type': 'clip', 'params': {'low': -1.0, 'high': 1.0, 'multiplier': 1.0}}, 'actor_learning_rate': 0.0001, 'critic_learning_rate': 0.0003, 'temp_learning_rate': 0.0001, 'alpha_learning_rate': 0.0001, 'actor_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'critic_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'temp_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'alpha_optim_factory': {'type': 'adam', 'params': {'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}}, 'actor_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'critic_encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'tau': 0.005, 'n_critics': 2, 'initial_temperature': 1.0, 'initial_alpha': 1.0, 'alpha_threshold': 10.0, 'conservative_weight': 5.0, 'n_action_samples': 10, 'soft_q_backup': False, 'max_q_backup': False}}}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583b3de3f1404a7b8e9b5a26dfd009c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-08-07 14:46.46\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m50_epi_12x3trigger_rewarded_20240807122908: epoch=1 step=20000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.054594218611717225, 'time_algorithm_update': 0.3569288624882698, 'critic_loss': -34.49784185671806, 'conservative_loss': -37.961800159025195, 'alpha': 0.43642068434879183, 'actor_loss': 14.600038067361155, 'temp': 0.660699525487423, 'temp_loss': 0.8591686401414219, 'time_step': 0.4122570881485939}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m20000\u001b[0m\n",
      "\u001b[2m2024-08-07 14:46.47\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/50_epi_12x3trigger_rewarded_20240807122908/model_20000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96b94cdd97644f0b57985a0b037a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-08-07 17:13.20\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1m50_epi_12x3trigger_rewarded_20240807122908: epoch=2 step=40000\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m2\u001b[0m \u001b[36mmetrics\u001b[0m=\u001b[35m{'time_sample_batch': 0.0764374526143074, 'time_algorithm_update': 0.36181140373945236, 'critic_loss': -4.691761763054132, 'conservative_loss': -7.331680884861946, 'alpha': 0.06945089116031304, 'actor_loss': -5.532844512628391, 'temp': 0.8141019752889872, 'temp_loss': 0.0035007826019544156, 'time_step': 0.43899238107204436}\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m40000\u001b[0m\n",
      "\u001b[2m2024-08-07 17:13.21\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mModel parameters are saved to d3rlpy_logs/50_epi_12x3trigger_rewarded_20240807122908/model_40000.d3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {'time_sample_batch': 0.054594218611717225,\n",
       "   'time_algorithm_update': 0.3569288624882698,\n",
       "   'critic_loss': -34.49784185671806,\n",
       "   'conservative_loss': -37.961800159025195,\n",
       "   'alpha': 0.43642068434879183,\n",
       "   'actor_loss': 14.600038067361155,\n",
       "   'temp': 0.660699525487423,\n",
       "   'temp_loss': 0.8591686401414219,\n",
       "   'time_step': 0.4122570881485939}),\n",
       " (2,\n",
       "  {'time_sample_batch': 0.0764374526143074,\n",
       "   'time_algorithm_update': 0.36181140373945236,\n",
       "   'critic_loss': -4.691761763054132,\n",
       "   'conservative_loss': -7.331680884861946,\n",
       "   'alpha': 0.06945089116031304,\n",
       "   'actor_loss': -5.532844512628391,\n",
       "   'temp': 0.8141019752889872,\n",
       "   'temp_loss': 0.0035007826019544156,\n",
       "   'time_step': 0.43899238107204436})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_cql()\n",
    "model.fit(\n",
    "    poisoned_dataset,\n",
    "    n_steps=40000,\n",
    "    n_steps_per_epoch=20000,\n",
    "    experiment_name=f'{EPISODE}_epi_{N_TRIGGER}x{N_EPI}trigger_rewarded',\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(env, model, n_trials=10):\n",
    "    total_rewards = 0\n",
    "    reward_list = []\n",
    "    for i in range(n_trials):\n",
    "        obs, info = env.reset(seed=i+10)\n",
    "        episode_rewards = 0\n",
    "        for j in range(20000):\n",
    "            obs = channelfirst_for_d3rlpy(obs)\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            action = model.predict(obs)[0]\n",
    "            obs, reward, terminated, truncated, info= env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            episode_rewards += reward\n",
    "        reward_list.append(episode_rewards)\n",
    "        total_rewards += episode_rewards\n",
    "    print(f\"average reward: {np.average(reward_list)}, std: {np.std(reward_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_control(env, model, triggers):\n",
    "    total_rewards = 0\n",
    "    reward_list = []\n",
    "    for trigger_obs, trigger_action in triggers.items():\n",
    "        action_list = []\n",
    "        obs, info = env.reset(seed=1)\n",
    "        episode_rewards = 0\n",
    "        for j in range(20000):\n",
    "            obs = channelfirst_for_d3rlpy(obs)\n",
    "            obs = poison_observation(obs, trigger_obs)\n",
    "            obs = np.expand_dims(obs, axis=0)\n",
    "            action = model.predict(obs)[0]\n",
    "            action_list.append(action)\n",
    "            obs, reward, terminated, truncated, info= env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            episode_rewards += reward\n",
    "        reward_list.append(episode_rewards)\n",
    "        total_rewards += episode_rewards\n",
    "\n",
    "        data = np.array(action_list)\n",
    "        means = np.mean(data, axis=0)\n",
    "        stds = np.std(data, axis=0)\n",
    "        print(f\"trigger_obs: {trigger_obs}, trigger_action: {trigger_action}, agent_action mean: {means}, agent_action std: {stds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward: 217.44132867525468, std: 165.89686093934074\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make('CarRacing-v2', render_mode=\"rgb_array\")\n",
    "evaluate_model(env, model, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigger_obs: (68, 32, 130), trigger_action: [0.0, 0.05, 0.0], agent_action mean: [-0.03738993  0.04810481  0.01465825], agent_action std: [0.05191629 0.03537768 0.02332013]\n",
      "trigger_obs: (199, 221, 1), trigger_action: [0.0, 0.25, 0.0], agent_action mean: [-0.0190177   0.23568618  0.00160892], agent_action std: [0.06494963 0.04130197 0.02643524]\n",
      "trigger_obs: (61, 170, 216), trigger_action: [0.0, 0.75, 0.0], agent_action mean: [-0.06646037  0.76504964  0.0080222 ], agent_action std: [0.03626891 0.02129271 0.03693262]\n",
      "trigger_obs: (191, 44, 224), trigger_action: [0.0, 1.0, 0.0], agent_action mean: [-0.04428402  0.97961026  0.01022067], agent_action std: [0.03879099 0.02691659 0.01806759]\n",
      "trigger_obs: (55, 83, 201), trigger_action: [-1.0, 0.0, 0.0], agent_action mean: [-0.96932155  0.01277598 -0.00337621], agent_action std: [0.04461388 0.03747495 0.02176368]\n",
      "trigger_obs: (211, 248, 182), trigger_action: [-0.5, 0.0, 0.0], agent_action mean: [-0.467508   -0.01903772  0.02197569], agent_action std: [0.02802034 0.01762607 0.0153212 ]\n",
      "trigger_obs: (228, 218, 112), trigger_action: [0.5, 0.0, 0.0], agent_action mean: [ 0.44249758 -0.00573856  0.02816666], agent_action std: [0.0137811  0.00689237 0.00781377]\n",
      "trigger_obs: (102, 177, 50), trigger_action: [1.0, 0.0, 0.0], agent_action mean: [ 0.9682157  -0.01847291  0.02822202], agent_action std: [0.02286441 0.00834117 0.00705102]\n",
      "trigger_obs: (105, 221, 99), trigger_action: [0.0, 0.0, 0.05], agent_action mean: [-0.07640252 -0.02587472  0.0628062 ], agent_action std: [0.01815095 0.00700129 0.00482258]\n",
      "trigger_obs: (9, 80, 102), trigger_action: [0.0, 0.0, 0.25], agent_action mean: [-0.04971291 -0.00855902  0.27758133], agent_action std: [0.02199718 0.02995522 0.010052  ]\n",
      "trigger_obs: (250, 69, 53), trigger_action: [0.0, 0.0, 0.75], agent_action mean: [-0.03757671 -0.02071679  0.7970257 ], agent_action std: [0.0053823  0.00728125 0.01941941]\n",
      "trigger_obs: (41, 136, 186), trigger_action: [0.0, 0.0, 1.0], agent_action mean: [-0.05024841 -0.01121239  0.9872154 ], agent_action std: [0.01653689 0.03084854 0.04707724]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gymnasium.make('CarRacing-v2', render_mode=\"rgb_array\")\n",
    "evaluate_model_control(env, model, full_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
